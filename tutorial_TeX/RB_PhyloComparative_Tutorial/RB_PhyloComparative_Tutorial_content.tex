\section{Introduction}

The subject of the comparative method is the analysis of trait evolution at the macroevolutionary scale.
In a comparative context, many different questions can be addressed: tempo and mode of evolution, correlated evolution of multiple quantitative traits, trends and bursts, changes in evolutionary mode correlated with major key innovations in some groups, etc \citep[for a good introduction see][]{Harvey1991}.

In order to correctly formalize comparative questions, the underlying phylogeny should always be explicitly accounted for. This point is clearly illustrated, in particular, by the independent contrasts method \citep{Felsenstein1985,Huelsenbeck2003}. Practically speaking, the phylogeny and the divergence times are usually first estimated using a separate phylogenetic reconstruction software. In a second step, this time-calibrated phylogeny is used as an input to the comparative method.
Doing this, however, raises a certain number of methodological problems:
\begin{itemize}
\item
the uncertainty about the phylogeny (and about divergence times) is ignored
\item
the traits themselves may have something to say about the phylogeny
\item
the rate of substitution, and more generally the parameters of the substitution process, can also be seen as quantitative traits, amenable to a comparative analysis.
\end{itemize}
All these points are not easily formalized in the context of the step-wise approach mentioned above.
Instead, what all this suggests is that phylogenetic reconstruction, molecular dating and the comparative method should all be considered jointly, in the context of one single overarching probabilistic model.

Thanks to its modular structure, \RevBayes~represents a natural framework for attempting this integration.
The aim of the present tutorial is to guide you through a series of examples where this integration is achieved, step by step.
It can also be considered as an example of the more general perspective of \emph{integrative modeling}, which can be recruited in many other contexts.



%%%%%%%%
%%   Data   %%
%%%%%%%%
\section{Data and files}

We provide several data files which we will use in this tutorial.
You may want to use your own data instead.
In the \cl{data} folder, you will find the following files
\begin{itemize}
\item
\cl{primates\_cytb.nex}: Alignment of the \textit{cytochrome b} subunit from 23 primates representing 14 of the 16 families (\textit{Indriidae} and \textit{Callitrichidae} are missing).

\item
\cl{primates\_lhtlog.nex}: 2 life-history traits (endocranial volume (ECV), body mass; each for males and females separately)  for 23 primate species \citep[taken from the Anage database,][]{DeMagalhaes2009}. The traits have been log-transformed.

\item
\cl{primates.tree}: A time calibrated phylogeny of the same 23 primates.
\end{itemize}





\section{Univariate Brownian evolution of quantitative traits}

\label{univariate}

As a first preliminary exercise, we wish to reconstruct the evolution of body mass in primates and, in particular, estimate the body mass of their last common ancestor.
For this, we will assume that the logarithm of body mass follows a simple univariate Brownian motion along the phylogeny.
In a first step, we will ignore phylogenetic uncertainty:
thus, we will assume that the Brownian process describing body mass evolution runs along a fixed time-calibrated phylogeny (with fixed divergence times), such as specified in the file \cl{primates.tree}.

\noindent \\ \impmark You may want to take the time to visualize the tree given in \cl{primates.tree} as well as the matrix of quantitative traits specified by the \cl{primates\_lhtlog.nex} file, before going into the modeling work described below.


\subsection{The model and the priors}

A univariate Brownian motion $x(t)$ is parameterized by its starting value at the root of the phylogeny $x(0)$ and a rate parameter $\sigma$. This rate parameter tunes the amplitude of the variation per unit of time. Specifically, along a given time interval $(0,T)$, the value of $X$ at time $T$ is normally distributed, with mean $x(0)$ and variance $\sigma^2 T$:
\begin{eqnarray*}
x(T) & \sim & \text{Normal} \left( x(0), \sigma^2 T \right).
\end{eqnarray*}

Concerning $\sigma$, we can formalize the idea that we are ignorant about the \emph{scale} (the order of magnitude) of this parameter by using a log-uniform prior:
\begin{eqnarray*}
\sigma &\sim& \frac{1}{\sigma}.
\end{eqnarray*}

Concerning the initial value $x(0)$ of the Brownian process at the root of the phylogeny.
Alternatively, you may want to specify a normal distribution as the prior distribution on the root value if you have some prior information.

Finally, the tree topology $\psi$ is, as mentioned above, fixed to some externally given phylogeny.
The entire model is now specified: tree $\psi$, variance $\sigma$ and Brownian process $x(t)$:
\begin{eqnarray*}
\sigma &\sim& \frac{1}{\sigma},
\\
x(0) &\sim& \text{Uniform},
\\
x(t) \mid \Psi, \sigma &\sim& \text{Brownian} \left( x(0), \, \psi, \, \sigma \right).
\end{eqnarray*}
Conditioning the model on empirical data by clamping $x(t)$ at the tips of the phylogeny, we can then run a MCMC to sample from the joint posterior distribution on $\sigma$ and $x$. Once this is done, we can obtain posterior means, medians or credible intervals for the value of body mass or other life-history traits for specific ancestors.


\subsection{Programming the model in \RevBayes}

The problem of continuous trait evolution ---just as for discrete trait evolution--- along a phylogeny is that we do not know the values of the traits at the internal nodes. That means, that we need to treat the states at the internal nodes as additional parameters of the model. For discrete characters we use the sum-product (a.k.a.~pruning) algorithm \citep{Felsenstein1981} to analytically integrate over all possible states at the internal nodes. For continuous characters (traits) similar methods have been proposed.
In \RevBayes~you have three main ways of specifying this model and running an analysis on it. The three approaches are: (1) phylogenetic independent contrasts using the reduced likelihood (REML), (2) Brownian motion using a phylogenetic covariance matrix, and (3) a full Brownian motion model using data augmentation.
Each of these approaches has there advantages and disadvantages as will be explained below.
Nevertheless, all approaches give the same results in terms of rate estimation.


\subsection{Phylogenetic Independent Contrasts using the reduced likelihood (REML)}

The reduced or restricted maximum likelihood (REML) method computes the probability of observing the continuous character at the tips by an analytical solution to integrate over the internal states \citep{Felsenstein1985}. 
This analytical solution is very fast to compute and thus can be applied to large phylogenies and/or many independent characters. However, the REML method looses the information about the location of the root state and thus you cannot infer which state the root or other internal nodes have.

You do not need to understand the algorithm but we provide a sketch of the idea behind REML to give you some insights.
REML compute the values at the internal nodes as the phylogenetic contrasts $x_k = x_i - x_j$ where $x_i$ and $x_j$ are the values of the child nodes in the phylogeny.
Then, we can compute the probability of observing the contrast $x_k$ using the probability density of a normal distribution with mean $\mu = 0$ and standard deviation $\sigma = \sqrt{\nu_i + \delta_i + \nu_j + \delta_j}$ where $\nu_i$ and $\nu_j$ are the (scaled) branch lengths leading to node $i$ and $j$ respectively. $\delta$ is the additional uncertainty that is propagated through the phylogeny  and is compute by $\delta_k = ((\nu_i + \delta_i)*(\nu_j + \delta_j)) / (\nu_i + \delta_i + \nu_j + \delta_j)$. These computations are done for you in the \RevBayes~distribution called \cl{dnPhyloBrownianREML}.

In the directory \cl{RevBayes\_scripts/} you will find a script called \cl{primatesMass\_BM\_REML.Rev}.
This script implements the univariate Brownian model described above. Instead of re-typing the content of script entirely in the context of an interactive \RevBayes~session, you can instead run the script directly:
{\tt \small \begin{snugshade*}
\begin{lstlisting}
source("RevBayes_scripts/primatesMass_BM_REML.Rev")
\end{lstlisting}
\end{snugshade*}}
This script essentially reformulates what has been explained in the last subsection and serves as an example solution for you. For the later section you need to adjust the script.

Let us go through the script step by step in the \Rev~language.
First, load the trait data:
{\tt \small \begin{snugshade*}
\begin{lstlisting}
contData <- readContinuousCharacterData("data/primates_lhtlog.nex")
\end{lstlisting}
\end{snugshade*}}
If you type you will see that the continuous character data matrix contains several characters (columns). 
{\tt \small \begin{snugshade*}
\begin{lstlisting}
contData
|*
|*   Continuous character matrix with 23 taxa and 11 characters
|*   ==========================================================
|*   Origination:                   primates_lhtlog.nex
|*   Number of taxa:                23
|*   Number of included taxa:       23
|*   Number of characters:          11
|*   Number of included characters: 11
|*   Datatype:                      Continuous
\end{lstlisting}
\end{snugshade*}}
Since we only want the body mass (of females) we exclude all but the third character
{\tt \small \begin{snugshade*}
\begin{lstlisting}
contData.excludeAll()
contData.includeCharacter(3) 
\end{lstlisting}
\end{snugshade*}}

Next, load the time-tree from file. Remember that we use in this first simple example a fixed tree that we assume is known without uncertainty.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
treeArray <- readTrees("data/primates.tree")
psi <- treeArray[1]
\end{lstlisting}
\end{snugshade*}}
\noindent \\ \impmark You may want to look at this tree before by loading the \cl{primates.tree} in FigTree or any other tree visualization software.

%Next we will specify some useful variables based on our dataset. The variable \cl{data} has \textit{member functions} that we can use to retrieve information about the dataset. 
%These include the number of species (\cl{n\_species}), the tip labels (\cl{names}), and the number of internal branches (\cl{n\_branches}).
%Each of these variables will be necessary for setting up different parts of our model.
%{\tt \begin{snugshade*}
%\begin{lstlisting}
%n_species <- data.ntaxa()
%names <- data.names()	
%n_branches <- 2 * n_species - 3 
%\end{lstlisting}
%\end{snugshade*}}

As usual, we start be initializing some useful helper variables.
%For example, we set up a counter variable for the number of moves that we already added to our analysis.
For example, we will set up a vector for the moves.
This will make it much easier if we extend the model or analysis to include additional moves or to remove some moves.
{\tt \begin{snugshade*}
\begin{lstlisting}
moves = VectorMoves() 
\end{lstlisting}
\end{snugshade*}}

Then, we define the overall rate parameter $\sigma$ which we assign a (truncated) log-uniform prior. Note that it is more efficient in Bayesian inference to specify a uniform prior and then to transform the parameter which we will use here:
{\tt \small \begin{snugshade*}
\begin{lstlisting}
logSigma ~ dnUniform(-5,5)
sigma := 10^logSigma
\end{lstlisting}
\end{snugshade*}}
Using this approach we have specified a prior probability distribution on \cl{sigma} between $10^{-5}$ to $10^5$ which should be broad enough to include all reasonable values.
%To accelerate convergence, it can be useful to force initialization of $\sigma$ to a small value:
%{\tt \small \begin{snugshade*}
%\begin{lstlisting}
%sigma.setValue(0.1)
%\end{lstlisting}
%\end{snugshade*}}

Since the rate of trait evolution \cl{logSigma} is a stochastic variable and we want to estimate it, we need to add a sliding move on it. Remember that the sliding move proposes new values drawn from a window with width \cl{delta} and is centered around the current values; thus it slides through the parameter space together with the current parameter value.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
moves.append( mvSlide(logSigma, delta=1.0, tune=true, weight=2.0) )
\end{lstlisting}
\end{snugshade*}}

Next, define a random variable from the univariate Brownian-Phylo-REML process, which we will call \cl{logmass}.
We need to provide the tree variable \cl{psi}, some branch-specific rate multiplier parameter which we simply set to 1, the shared rate for this site \cl{sigma} and the number of sites (number of continuous traits) that we use. 
{\tt \small \begin{snugshade*}
\begin{lstlisting}
logmass ~ dnPhyloBrownianREML(psi, branchRates=1.0, siteRates=sigma, nSites=1)
\end{lstlisting}
\end{snugshade*}}
Now, condition the Brownian model on empirically observed values for body mass in the extant taxa.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
logmass.clamp( contData )
\end{lstlisting}
\end{snugshade*}}

The model is now entirely specified and we can create a model object containing the entire model graph by providing it with only one of our model variables, \EG \cl{sigma}. 
{\tt \small \begin{snugshade*}
\begin{lstlisting}
mymodel = model(sigma)
\end{lstlisting}
\end{snugshade*}}

To see what it happing during the MCMC let us make a screen monitor that tracks the rate \cl{sigma}.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
monitors[1] = mnScreen(printgen=10, sigma)
\end{lstlisting}
\end{snugshade*}}

Additionally, we'll use a file monitor that does the same thing, but directly stores the values into a file.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
monitors[2] = mnFile(filename="output/primates_mass_REML.log", printgen=10, separator = TAB, sigma)
\end{lstlisting}
\end{snugshade*}}

We can finally create a mcmc, and run it for a good 100 000 cycles after we did a burnin phase of 10 000 iterations:
{\tt \small \begin{snugshade*}
\begin{lstlisting}
mymcmc = mcmc(mymodel, monitors, moves)
mymcmc.burnin(generations=10000,tuningInterval=500)
mymcmc.run(100000)
\end{lstlisting}
\end{snugshade*}}




\subsection*{Exercises}

\begin{itemize}
\item
Run the model.
\item
using \cl{Tracer}, visualize the posterior distribution on the rate parameter \cl{sigma}
\item
calculate the 95\% credible interval for the rate of evolution of the log of body mass ($\sigma$)
\end{itemize}


\vspace{5cm}






\subsection{Phylogenetic covariance matrix}

The second method that we will use creates a phylogenetic covariance matrix. The phylogenetic covariance matrix method integrates over the states  at the internal nodes as well but uses instead a multivariate normal distribution.
The key advantage is that this method provides information about the root state since it models the root state as an additional parameter of the model. The disadvantage is that it is very computationally intensive. That means, that the phylogenetic covariance matrix approach may take long for very large data sets (at least in its current implementation).

\noindent \\ \impmark Copy the file \cl{primatesMass\_BM\_REML.Rev}, name it for example \cl{primatesMass\_BM\_Cov.Rev} and start editing it.

In the previous example, the REML approach, we did not specify a parameter for the state at the root.
In this exercise, we need this additional parameter.
Let us use a uniform prior distribution on the logarithm of the root mass.
A uniform prior between -100 and 100 should be diffuse enough. Just image how big or small an individual needs to be if it has body mass smaller than exp(-100) or larger than exp(100).
{\tt \small \begin{snugshade*}
\begin{lstlisting}
rootlogmass ~ dnUniform(-100,100)
\end{lstlisting}
\end{snugshade*}}
Next, we'll specify a sliding move that proposes new values for the \cl{rootlogmass} randomly drawn from a window centered around the current value.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
moves.append( mvSlide(rootlogmass,delta=10,tune=true,weight=2)  )
\end{lstlisting}
\end{snugshade*}}

Finally, we need to substitute the \cl{dnPhyloBrownianREML} by \cl{dnPhyloBrownianMVN} to use the phylogenetic covariance matrix approach.
Again, we provide the tree variable \cl{psi}, some branch-specific rate multiplier parameter which we simply set to 1, the shared rate for this site \cl{sigma} and the number of sites (number of continuous traits) that we use. 
{\tt \small \begin{snugshade*}
\begin{lstlisting}
logmass ~ dnPhyloBrownianMVN(psi, branchRates=1.0, siteRates=sigma, rootStates=rootlogmass, nSites=1)
\end{lstlisting}
\end{snugshade*}}
This will automatically connect the parameters of the model together.

Additionally, we now have the extra parameter \cl{rootlogmass} which we want to monitor.
Thus we need to replace the file monitor and use instead.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
monitors[2] = mnFile(filename="output/primates_mass_Cov.log", printgen=10, separator = TAB, sigma, rootlogmass)
\end{lstlisting}
\end{snugshade*}}


\noindent \\ \impmark Don't forget to change the output file names in the monitors, otherwise your old analyses files will be overwritten.


\subsection*{Exercises}

\begin{itemize}
\item
Run the analysis.
\item
Using \cl{Tracer}, visualize the posterior distribution on the rate parameter \cl{sigma} and the \cl{rootlogmass}
\item 
How does the posterior distribution of \cl{sigma} looks compared with the first analysis?
\item
Calculate the 95\% credible interval for the rate of evolution of the log of body mass ($\sigma$) and the \cl{rootlogmass}
\end{itemize}

\vspace{5cm}








\subsection{Data augmentation}

The third method to we will use is a data augmentation method. The data augmentation method uses explicitly the states at the internal nodes. We will specify the full model in \Rev.
That means that we will create a random variable for each node of the tree using a for loop. Each trait is then simply assign a normal distribution, as we described above in the model description.
The advantage of this method is that you estimate the values at the internal nodes directly and that you have full control about modifying any part of the model. The disadvantage is that the MCMC algorithm will be harder if you want to jointly estimate the phylogeny, although the likelihood computation is very fast. The problem is the mixing of the MCMC because proposing new trees involves proposing new good values for the states at the internal nodes.

\noindent \\ \impmark Copy the file \cl{primatesMass\_BM\_REML.Rev}, name it for example \cl{primatesMass\_BM\_DA.Rev} and start editing it.

]As in the previous example we will use a uniform prior distribution on the logarithm of the root mass.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
rootlogmass ~ dnUniform(-100,100)
\end{lstlisting}
\end{snugshade*}}
Again, we'll specify a sliding move that proposes new values for the \cl{rootlogmass} randomly drawn from a window centered around the current value.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
moves.append( mvSlide(rootlogmass,delta=10,tune=true,weight=2)  )
\end{lstlisting}
\end{snugshade*}}

In order to create the random variables for the internal states we need to know the number of nodes and the number of tips.
We will store these as some helper variables.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
numNodes = psi.nnodes()
numTips = psi.ntips()
\end{lstlisting}
\end{snugshade*}}

Now we are ready to specify the Brownian motion model for each branch.
That is, we simply specify a new normal distributed random variable for each node with mean being equal to the value of the parent variable and the standard deviation being equal to the product of the square root of the branch length and our rate parameter \cl{sigma}. We store all the variables in the vector \cl{logmass}. Then we are able to access the value at the parent node using the index of the parent node, which we can obtain from the tree using the function \cl{psi.parent(i)}. Similarly, since the variance depends on the branch length we retrieve the branch length of node with index \cl{i} using the function \cl{psi.branchLength(i)}.

First we need to copy (create a reference to) the \cl{rootlogmass}
{\tt \small \begin{snugshade*}
\begin{lstlisting}
logmass[numNodes] := rootlogmass
\end{lstlisting}
\end{snugshade*}}
Let us start by creating the random variables for the internal nodes. Remember that the variance is equal to \cl{sigma}-squared times the branch length, and we need to compute the square root of it to obtain the standard deviation.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
# univariate Brownian process along the tree
# parameterized by sigma
for (i in (numNodes-1):(numTips+1) ) {
  logmass[i] ~ dnNormal( logmass[psi.parent(i)], sd=sigma*sqrt(psi.branchLength(i)) )
  # moves on the Brownian process
  moves.append( mvSlide( logmass[i], delta=10, tune=true ,weight=2)  )
}
\end{lstlisting}
\end{snugshade*}}
You may have noticed that we specified in the loop a move for each internal \cl{logmass}. This is because we want to use the MCMC algorithm to integrate over the uncertainty in the states.

Next, we repeat the same loop but now for the tip nodes. Instead of applying a move to each tip node we will clamp the nodes. The nodes will be clamped with the data that we read in before.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
for (i in numTips:1 ) {
  logmass[i] ~ dnNormal( logmass[psi.parent(i)], sd=sigma*sqrt(psi.branchLength(i)) )

  # condition Brownian model on quantitative trait data (second column of the dataset)
  logmass[i].clamp(contData.getTaxon(psi.nodeName(i))[1])
}
\end{lstlisting}
\end{snugshade*}}
Here we do not need a \cl{dnPhyloBrownianREML} or \cl{dnPhyloBrownianMVN} distribution anymore because we explicitly instantiated the model. Note that the approach we have taken using the loop is the tree-plate approach described in \citep{Hohna2014b}

Since we have several additional parameters ---the states at the internal nodes--- we will use a model monitor to write to file instead.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
monitors[2] = mnModel(filename="output/primates_mass_DA.log", printgen=10, separator = TAB)
monitors[3] = mnExtNewick(filename="output/primates_mass_DA_ext.trees", isNodeParameter=TRUE, printgen=10, separator = TAB, tree=psi, logmass)
\end{lstlisting}
\end{snugshade*}}

To get the annotate tree we use the map tree function.

{\tt \small \begin{snugshade*}
\begin{lstlisting}
treetrace = readTreeTrace("output/primates_mass_DA_ext.trees", treetype="clock")
map_tree = mapTree(treetrace,"output/primates_mass_DA_ext_MAP.tree")
\end{lstlisting}
\end{snugshade*}}


\noindent \\ \impmark Don't forget to change the output file names in the monitors, otherwise your old analyses files will be overwritten.


\subsection*{Exercises}

\begin{itemize}
\item
Run the analysis.
\item
Using \cl{Tracer}, visualize the posterior distribution on the rate parameter \cl{sigma} and the \cl{rootlogmass} and the internal states.
\item 
How does the posterior distribution of \cl{sigma} looks compared with the first and second analysis?
\item
Calculate the 95\% credible interval for the rate of evolution of the log of body mass ($\sigma$) and the \cl{rootlogmass}. Have they changed?
\end{itemize}

\vspace{5cm}



\subsection{Brief intermediate summary}
In the above exercises you have analyzed the log-transformed body mass using a Brownian motion (BM) along a phylogeny. We assumed the phylogeny to be known without error and were primarily interested in the rate how fast the body mass evolves. The exercises showed you three different ways how to approach the BM model, each having its advantages and disadvantages. You will need to decide for your analysis which approach is most appropriate. Our intention was mainly to show you some of the flexibility in \RevBayes~how to specify the same model in many different ways, exposing sometimes more and sometimes less of the internal model graph structure. As an extra exercise you could start thinking about how to extend the basic BM.



\vspace{5cm}


\section{Multiple independently evolving traits}
The Brownian motion can easily be applied to multiple traits. The exactly same procedure and model will be applied as above and thus we will skip the repetitive description. The interesting questions that you can ask about multiple traits is ---amongst others--- if rates between traits vary.


As an example we use now the first four characters of the data matrix, which are: 1) female log endocranial volume (ECV), 2) male log ECV, 3) female log body mass and 4) male log body mass.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
contData <- readContinuousCharacterData("data/primates_lhtlog.nex")
contData.excludeCharacter(5:11)
\end{lstlisting}
\end{snugshade*}}
In the first simple example we assume that all site rates are equal, thus we use a rate multiplier we we call \cl{perSiteRates}.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
perSiteRates <- [1,1,1,1]
\end{lstlisting}
\end{snugshade*}}
Furthermore, we have now four characters and hence we will estimate a root state for each one of them. We use the same prior probability for every character, just as above for the single character example.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
for (i in 1:4) {
   rootlogmass[i] ~ dnUniform(-100,100)
   moves.append( mvSlide(rootlogmass[i],delta=10,tune=true,weight=2)  )
}
\end{lstlisting}
\end{snugshade*}}

Finally, we bring together all the parameter to specify our Brownian motion model along the tree. Note that we specify here the site rates as the product of the global rate \cl{sigma} time the sites specific rates (which are, however all equal in this first exercie).
{\tt \small \begin{snugshade*}
\begin{lstlisting}
logmass ~ dnPhyloBrownianMVN(psi, branchRates=1.0, siteRates=sigma*perSiteRates, rootStates=rootlogmass, nSites=4)
\end{lstlisting}
\end{snugshade*}}

After this you need to create the model using the \cl{model} function, create your monitors, create the MCMC and finaly run the MCMC.


\subsection*{Exercises}

\begin{itemize}
\item
Run the analysis.
\item
Using \cl{Tracer}, visualize the posterior distribution on the rate parameter \cl{sigma} and the four \cl{rootlogmass} values.
\item 
Compare the results to the previous single trait analyses?
\end{itemize}

\vspace{5cm}



\subsection{Independent site rates}

{\tt \small \begin{snugshade*}
\begin{lstlisting}
perSiteRates ~ dnDirichlet([1,1,1,1])
moves.append( mvSimplexElementScale(perSiteRates,alpha=10,tune=true,weight=4) )
\end{lstlisting}
\end{snugshade*}}


\subsection*{Exercises}

\begin{itemize}
\item
Run the analysis.
\item
Using \cl{Tracer}, visualize the posterior distribution on the per site rate parameter \cl{perSiteRates}.
\item 
How are the estimate per site rates compared to each other?
\end{itemize}

\vspace{5cm}


\subsection{Partially shared site rates}
We hope that your results in the previous analysis showed that the rates for females and males for both the ECV and the body mass are very similar.
This motivates us to specify an explicit model where the rates between ECV and body mass evolution are different but shared between females and males.
We will model the difference using the \cl{siteRateDiff} parameter which is drawn from a Beta(1,1) distribution, which is essentially a uniform distribution between 0 and 1.
We still choose the Beta distribution because it is easier to specify more informative priors.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
siteRateDiff ~ dnBeta(1,1)
\end{lstlisting}
\end{snugshade*}}
Next, we specify a sliding move on the \cl{siteRateDiff}.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
moves.append( mvSlide(siteRateDiff,delta=10,tune=true,weight=2) )
\end{lstlisting}
\end{snugshade*}}
To obtain the rates we use \cl{siteRateDiff} as the rate for the ECV evolution in females and males and 1-\cl{siteRateDiff} as the rate for the body mass evolution in females and males.
Note, however, that we need a small workaround here. \Rev~is a strictly typed language, which you may have noticed. The per site rates need to be positive real number (negative rates obviously don't make sense). However, the difference between two number is not guaranteed to be positive, only if we know that the first term is larger than the second. This is exactly the case for 1-\cl{siteRateDiff} but \RevBayes~doesn't know this so we need to tell it by using the absolute value \cl{abs} function.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
perSiteRates[1] := siteRateDiff
perSiteRates[2] := siteRateDiff
perSiteRates[3] := abs(1.0 - siteRateDiff)   # specify the type here
perSiteRates[4] := abs(1.0 - siteRateDiff)
\end{lstlisting}
\end{snugshade*}}
You could have specified many other prior distribution on the relative rate (or ratio) such as a uniform prior distribution and then use logit or hyperbolic tangent transformation.
The only important factor is that the rates are somehow normalized because we use the global, shared rate \cl{sigma}.
Using the beta distribution makes the rates automatically normalized because the two different rates sum to one. Note that in the previous all four rates summed to 1.0 and here the two different rates sum to 1.0. This has the effect that we should estimate the global rate \cl{sigma} to be half of what you estimated previously. You may want to check this once you ran the analysis and loaded the output into \Tracer. 


The remaining functions are the same as in the previous example.


\subsection*{Exercises}

\begin{itemize}
\item
Run the analysis.
\item
Using \cl{Tracer}, visualize the posterior distribution on the per site rate parameter \cl{perSiteRates}.
\item 
Does the value of equal rate (\cl{siteRateDiff}=0.5) fall into the 95\% credible interval?
\item
If the rate does not fall into the 95\% credible interval, should we reject the hypothesis that the underlying rates are equal?
\end{itemize}

\vspace{5cm}






\subsection{Model selection}
Our previous analysis explored the parameters of the rate of evolution. You will have seen that the rate of evolution was smaller for the ECV compared with the rate of evolution of body size. However, we did not perform a statistical test to reject the hypothesis if the underlying rates between the different characters are significantly different. Therefore, we will perform a marginal likelihood estimation for all three model: 1) the equal rates, 2) the independent rates, and 3) the shared rates.

\noindent \\ \impmark You need to adopt the three scripts of your previous analysis.

Previously you created an \cl{mymcmc} object using the \cl{mcmc} function. Now, you need to replace this object with the power posterior MCMC object.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
pow_p = powerPosterior(mymodel, moves, monitors, "output/pow_p_BM_equal.out", cats=100, sampleFreq=10) 
\end{lstlisting}
\end{snugshade*}}
Next, you need to run the burnin and afterwards run the power posterior sampler. This will create 101 output files (100 stepping stones and one for the posterior) logging the values into the monitors that you have specified before (we recommend you not to use a screen monitor). You could look into each of these files to check that you obtain sufficiently many samples for each stone.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
pow_p.burnin(generations=10000,tuningInterval=250)
pow_p.run(generations=10000)  
\end{lstlisting}
\end{snugshade*}}
Use stepping-stone sampling to calculate marginal likelihoods.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
ss = steppingStoneSampler(file="output/pow_p_MultiBM_equal.out", powerColumnName="power", likelihoodColumnName="likelihood")
ss.marginal() 
\end{lstlisting}
\end{snugshade*}}
\RevBayes~will print to the screen the estimated marginal likelihood. Write down this value and repeat the exercise for the other two models.



\subsection*{Exercises}

\begin{itemize}
\item
Run the three different marginal likelihood estimations and write down the marginal likelihood.
\item
Compute the log-Bayes factors by taking the difference of the alternative model (independent rates \& shared rates) and the null hypothesis (equal rates).
\item
Is any of the log-Bayes factors larger than 1.16 for substantial support of the alternative hypothesis (or 2.3 for strong support)?
\end{itemize}

\vspace{5cm}





%\subsection{Beyond Brownian models}
%
%Throughout this tutorial, we have exclusively considered undirected Brownian models.
%However, many other models could be used,
%and this,
%both for quantitative traits and for substitution rates or substitution parameters.
%Right now, there are at least two other models available in \RevBayes:
%the Brownian model with systematic trend and the Ornstein-Uhlenbeck process.
%
%One possible application of the Brownian model with trend would be to test for the existence of a systematic trend in increasing body size (i.e. Cope's rule) during animal, vertebrate or mammalian evolution \citep{Alroy1998}. Note, however, that systematic trends cannot be estimated using only extant taxa (at least using purely anagenetic processes of evolution, such as considered here): the model would not be identifiable.
%If we have fossil data, on the other hand, we can estimate a trend:
%the model will then essentially rely on the average-mass-through-time distribution across the entire geological range.
%
%Technically, to model body size evolution with drift, we would just need to:
%
%define a drift parameter, with a diffuse prior centered on 0:
%{\tt \small \begin{snugshade*}
%\begin{lstlisting}
%copestrend ~ dnNorm(0,10)
%\end{lstlisting}
%\end{snugshade*}}
%create a (univariate) Brownian motion with drift:
%{\tt \small \begin{snugshade*}
%\begin{lstlisting}
%logmass ~ dnBrownian(psi,sigma,drift=copestrend)
%\end{lstlisting}
%\end{snugshade*}}
%move the trend parameter during the MCMC, using a regular sliding move:
%{\tt \small \begin{snugshade*}
%\begin{lstlisting}
%moves.append( mvSlide(copestrend, delta=2.0, tune=true, weight=3.0) )
%\end{lstlisting}
%\end{snugshade*}}
%
%After running the model, the posterior distribution on Cope's trend parameter can be visualized and quantified, and the empirical support in favor of Cope's rule can be assessed by estimating the posterior probability that this trend parameter is positive.








\section{Estimating the phylogeny from continuous character data}
In the motivation we argued that the phylogeny should be estimated together with the parameters of the evolutionary process. Only for simplicity we used fixed trees in the previous exercises.
Instead of using a tree fixed to a pre-defined value, the tree should now be moved during the MCMC.
Implementing this joint model in \RevBayes~is just a matter of adding the following features to the model defined in the previous section (after duplicating the script).
You may want to use the REML approach for computational efficient (\IE good MCMC mixing) although the phylogenetic covariance matrix and the data augmentation are generally possible too.


We need to get some useful variables from the data so that we will be able to specify the tree prior below. These variables are the number of tips, the number of nodes and the names of the species which we all can query from the continuous character data object.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
numTips = contData.ntaxa()
names = contData.names()
numNodes = numTips * 2 - 1
\end{lstlisting}
\end{snugshade*}}

Instead of having a fixed tree as in the previous, we should now define a \emph{random} tree. We use a birth death prior with prior distributions on the \cl{diversification} rate and \cl{turnover} rate.
The \cl{diversification} rate corresponds to the rate of growth of the tree and the \cl{turnover} rate corresponds to the rate how quickly a species is replaced by a new one.
This parametrization has the advantage that we can use meaningful prior distributions from the fossil record.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
diversification ~ dnLognormal(0,1)
turnover ~ dnGamma(4,4)
\end{lstlisting}
\end{snugshade*}}
Then, we compute deterministically the \cl{speciation} and \cl{extinction} rate from the \cl{diversification} rate and \cl{turnover} rate.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
speciation := diversification + turnover
extinction := turnover
\end{lstlisting}
\end{snugshade*}}
Instead, you could also use directly the speciation and extinction rates, \EG endowed with some diffuse exponential prior.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
# rescaling moves on speciation and extinction rates
moves.append( mvScale(diversification, lambda=1, tune=true, weight=3.0) )
moves.append( mvScale(turnover, lambda=1, tune=true, weight=3.0) )
\end{lstlisting}
\end{snugshade*}}
The phylogeny that we used are obviously not a complete sample of all the species and you should take the incomplete sampling into account. We will simply use an empirical estimate of the fraction of species which we included in this study. For more information about incomplete taxon sampling see \cite{Hohna2011} and \cite{Hohna2014a}. 
{\tt \small \begin{snugshade*}
\begin{lstlisting}
sampling_fraction <- 23 / 270     # 23 out of the ~ 270 primate species
\end{lstlisting}
\end{snugshade*}}
Now we are able to specify of tree variable \cl{psi} which is drawn from a constant rate birth-death process. We will condition the age of the tree to be 75 million years old which is approximately the crown age of primates, although this estimate is still debated. We only condition here on the crown age for simplicity because we do not use any other fossil calibration.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
psi ~ dnBDP(lambda=speciation, mu=extinction, rho=sampling_fraction, rootAge=75, nTaxa=numTips, names=names)
\end{lstlisting}
\end{snugshade*}}
Note that, here, we do not have included any fossil information: we are merely doing \emph{relative} dating. 

The first moves on the tree which we specify are moves that change the node ages. The first move randomly picks a subtree and rescales it, and the second move randomly pick a node and uniformly proposes a new node age between its parent age and oldest child's age.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
moves.append( mvSubtreeScale(psi, weight=5.0) )
moves.append( mvNodeTimeSlideUniform(psi, weight=10.0) )
\end{lstlisting}
\end{snugshade*}}

We also need moves on the tree topology to estimate the phylogeny. The two moves which you use are the nearest-neighbor interchange (NNI) and the fixed-nodeheight-prune-and-regraft (FNPR) \citep{Hohna2012}.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
moves.append( mvNNI(psi, weight=5.0) )
moves.append( mvFNPR(psi, weight=5.0) )\end{lstlisting}
\end{snugshade*}}
We are essentially done now. We  only need to add a new monitor for the tree so that we can monitor and build the maximum a posteriori tree later.
{\tt \small \begin{snugshade*}
\begin{lstlisting}
monitors[3] = mnFile(filename="output/primates_mass_multiBM_tree.trees", printgen=100, separator = TAB, psi)
\end{lstlisting}
\end{snugshade*}}

\noindent \\ \impmark A short analysis of 50 000 iterations will be sufficient.


\subsection*{Exercises}

\begin{itemize}
\item
Run the analysis.
\item
Create the maximum a posteriori (MAP) tree \cl{readTreeTrace} and \cl{mapTree}.
\item
Look at the estimated tree and compare it to the tree you used before.
\item 
How does the posterior distribution of \cl{sigma} looks compared with the first and second analysis?
\item
Compute the posterior probabilities of each tree using the \cl{treetrace.summarize()} command.
\item
What is the posterior probability of the best tree?
\end{itemize}

\vspace{5cm}







\section{Including information about the tree from molecular data}

Starting from the model implemented in the last section, we now want to account for phylogenetic uncertainty using information contained in molecular data. As first pointed out by \cite{Huelsenbeck2003}, this can easily be done in a Bayesian framework, through the use of a joint model combining sequence data and quantitative traits. Specifically:
\begin{itemize}
\item
two data sets are loaded: one for sequence data and one for quantitative traits
\item
a tree is defined under birth-death process
\item
a Brownian model is defined over the tree (just as described in the previous section)
\item
the Brownian model is conditioned on the quantitative trait data
\item
a substitution model is defined over the same tree (GTR+Gamma)
\item
the substitution model is conditioned on the molecular sequence data.
\end{itemize}
%Ideally, we would like to move both the topology and the divergence times.
%Mixing over tree topologies under a Brownian model is relatively challenging, however (it works, but it requires rather long MCMC runs).
%For that reason, in the following, we will mix over divergence times only,bunder the constraint of a fixed tree topology.
%The features of the model that would need to be modified in order to also mix over topologies will nevertheless be indicated. You may want try them after the workshop.

\subsection{Programming the model in \RevBayes}

First, we create a substitution model, just like what you probably did in previous tutorial (\EG RB\_CTMC\_Tutorial). 
In a first step, we will use a GTR+Gamma model.
Start by loading the sequence data matrix specified in \cl{data/primates\_cytb.nex}.
{\tt \begin{snugshade*}
\begin{lstlisting}
seqData <- readDiscreteCharacterData("data/primates_cytb.nex")
\end{lstlisting}
\end{snugshade*}}
We can use a flat Dirichlet prior density on the exchangeability rates \cl{er} and the the base frequencies \cl{pi}.
{\tt \begin{snugshade*}
\begin{lstlisting}
er_prior <- v(1,1,1,1,1,1) 
er ~ dnDirichlet(er_prior)
pi_prior <- v(1,1,1,1) 
pi ~ dnDirichlet(pi_prior)
\end{lstlisting}
\end{snugshade*}}
Now add the simplex scale move one each the exchangeability rates \cl{er} and the stationary frequencies \cl{pi} to the moves vector:
{\tt \small \begin{snugshade*}
\begin{lstlisting}
moves.append( mvSimplexElementScale(er)  )
moves.append( mvSimplexElementScale(pi)   )
\end{lstlisting}
\end{snugshade*}}
We can finish setting up this part of the model by creating a deterministic node for the GTR instantaneous-rate matrix \cl{Q}. 
The \cl{fnGTR()} function takes a set of exchangeability rates and a set of base frequencies to compute the instantaneous-rate matrix used when calculating the likelihood of our model.
{\tt \begin{snugshade*}
\begin{lstlisting}
Q := fnGTR(er,pi)
\end{lstlisting}
\end{snugshade*}}
The next part of the substitution process is the rate variation among sites. We will model this using the commonly applied 4 discrete gamma categories which only have a single parameter \cl{alpha}.
Let us specify the rate of \cl{alpha} to 0.05 (thus the mean will be 20.0).
{\tt\begin{snugshade*}
\begin{lstlisting}
alpha_prior <- 0.05                                                                             
\end{lstlisting}
\end{snugshade*}}
Then create a stochastic node called \cl{alpha} with an exponential prior:
{\tt\begin{snugshade*}
\begin{lstlisting}
alpha ~ dnExponential(alpha_prior)
\end{lstlisting}
\end{snugshade*}}
Initialize the \cl{gamma\_rates} deterministic node vector using the  \cl{fnDiscretizeGamma()} function with \cl{4} bins:
{\tt \begin{snugshade*}
\begin{lstlisting}
gamma_rates := fnDiscretizeGamma( alpha, alpha, 4 )
\end{lstlisting}
\end{snugshade*}}
The random variable that controls the rate variation is the stochastic node \cl{alpha}. 
We will apply a simple scale move to this parameter.
{\tt \begin{snugshade*}
\begin{lstlisting}
moves.append( mvScale(alpha, weight=2.0) )
\end{lstlisting}
\end{snugshade*}}
This finishes the substitution process part of the model.

Then next part of the model is the clock model. Here we need a clock model because we work on a time tree. We use our exponentiated uniform distribution to specify a flat prior distribution which is scale invariant.
{\tt \begin{snugshade*}
\begin{lstlisting}
logClockRate ~ dnUniform(-5,5)
clockRate := 10^logClockRate
\end{lstlisting}
\end{snugshade*}}
We will apply a sliding window move to the \cl{\cl{logClockRate}}.
{\tt \begin{snugshade*}
\begin{lstlisting}
moves.append( mvSlide(logClockRate, delta=0.1, tune=true, weight=2.0) )
\end{lstlisting}
\end{snugshade*}}

Remember that you need to call the \cl{PhyloCTMC} constructor to include the new site-rate parameter:
% This sentence was incomplete previously; I think the revised version is correct.
{\tt \begin{snugshade*}
\begin{lstlisting}
seq ~ dnPhyloCTMC(tree=psi, Q=Q, siteRates=gamma_rates, branchRates=clockRate, type="DNA")
\end{lstlisting}
\end{snugshade*}}
Finally we need to attach the molecular sequence data to our model.
{\tt \begin{snugshade*}
\begin{lstlisting}
seq.clamp(seqData)
\end{lstlisting}
\end{snugshade*}}
You may have wondered how the continuous trait model and the molecular sequence model are combined.
This happened automatically when you used the same tree parameter \cl{psi} for both models. 
Since both models share now the same parameter, they will be used for a joint analyses.
The model function thus will construct the joint model graph for the continuous trait model as well as the molecular sequence model.

After you ran the MCMC analyses, read in the tree trace. We will assume that you used a tree monitor and called the file \emph{output/primates\_joint.trees}. Change the name if you used a different one.
{\tt \begin{snugshade*}
\begin{lstlisting}
treetrace = readTreeTrace("output/primates_joint.trees", "clock")
\end{lstlisting}
\end{snugshade*}}
Then build the maximum a posteriori tree.
{\tt \begin{snugshade*}
\begin{lstlisting}
map = mapTree( file="primates_joint.tree", treetrace )
\end{lstlisting}
\end{snugshade*}}


Write this model and make sure that it runs when you give it to \RevBayes.


\subsection*{Exercises}

\begin{itemize}
\item
Run the analysis.
\item
Using \cl{Tracer}, visualize the posterior distribution on the rate parameter \cl{sigma} and compare it to the previous analyses.
\item 
Look at the MAP tree which you estimated now.
\end{itemize}

\vspace{5cm}





%\section{Autocorrelated relaxed molecular clock}
%
%In the previous model, no consideration was given to the problem of rate variation among lineages --
%we bluntly used a strict clock. This is of course problematic, in particular at the phylogenetic scale considered here ($\approx$ 75 million years), where we know that there is substantial rate variation. In addition, we know that substitution rates across branches are \emph{auto-correlated} in the present case.
%: typically, entire orders, such as rodents, are fast evolving, whereas other orders like Cetartiodactyla are slowly evolving.
%In other words, nearby lineages along the phylogeny tend to be characterized by similar substitution rates.
%
%You have perhaps already seen an autocorrelated relaxed clock model in the molecular dating session (ACLN). You could easily recruit it in the present context (a good exercise to try after the workshop: modify the model suggested in the previous section so as to replace the strict clock by the ACLN model).
%
%Here, however, we will derive the autocorrelated clock in a slightly different way. This derivation will be less straightforward, but more useful for what we want to do next.
%Specifically, we will first model the logarithm of the instant substitution rate as a Brownian motion, just like we did for body mass in section \ref{univariate}. Then, we will exponentiate this Brownian process and take branch-specific averages, which we will finally plug into the substitution model as the \cl{branchRates} argument.
%
%\noindent \\ \impmark Hint: You can simply use a Brownian motion for the \cl{logbranchrates} and then exponentiate the rates (\cl{branchrates := exp(logbranchrates)}).



\bibliographystyle{sysbio}
\bibliography{\GlobalResourcePath refs}
